{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2903d80e",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [3]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566b02a9",
   "metadata": {
    "papermill": {
     "duration": 0.005348,
     "end_time": "2023-08-12T09:30:47.066990",
     "exception": false,
     "start_time": "2023-08-12T09:30:47.061642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Input HMP\n",
    "This notebook pulls the HMP accelerometer sensor data classification data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72a15abc",
   "metadata": {
    "papermill": {
     "duration": 0.156835,
     "end_time": "2023-08-12T09:30:47.226510",
     "exception": false,
     "start_time": "2023-08-12T09:30:47.069675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e82e4184",
   "metadata": {
    "papermill": {
     "duration": 0.012447,
     "end_time": "2023-08-12T09:30:47.243272",
     "exception": false,
     "start_time": "2023-08-12T09:30:47.230825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path and file name for output (default: data.csv)\n",
    "data_csv = os.environ.get('data_csv', 'data.csv')\n",
    "\n",
    "# url of master (default: local mode)\n",
    "master = os.environ.get('master', \"local[*]\")\n",
    "\n",
    "# temporal data storage for local execution\n",
    "data_dir = os.environ.get('data_dir', 'data/')\n",
    "\n",
    "# sample on input data to increase processing speed 0..1 (default: 1.0)\n",
    "sample = os.environ.get('sample', '1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a44e74",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b86d972",
   "metadata": {
    "papermill": {
     "duration": 0.014452,
     "end_time": "2023-08-12T09:30:47.260436",
     "exception": true,
     "start_time": "2023-08-12T09:30:47.245984",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# override parameters received from a potential call using %run magic\n",
    "parameters = list(\n",
    "    map(\n",
    "        lambda s: re.sub('$', '\"', s),\n",
    "        map(\n",
    "            lambda s: s.replace('=', '=\"'),\n",
    "            filter(\n",
    "                lambda s: s.find('=') > -1,\n",
    "                sys.argv\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "for parameter in parameters:\n",
    "    exec(parameter)\n",
    "\n",
    "# cast parameters to appropriate type\n",
    "sample = float(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025a2240",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Lets create a local spark context (sc) and session (spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928c1b1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(master))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9714dbc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Lets pull the data in raw format from the source (github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58868b7c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -Rf HMP_Dataset\n",
    "!git clone https://github.com/wchill/HMP_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea54b5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"x\", IntegerType(), True),\n",
    "    StructField(\"y\", IntegerType(), True),\n",
    "    StructField(\"z\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c07035",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "This step takes a while, it parses through all files and folders and creates a temporary dataframe for each file which gets appended to an overall data-frame \"df\". In addition, a column called \"class\" is added to allow for straightforward usage in Spark afterwards in a supervised machine learning scenario for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62782063",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = 'HMP_Dataset/'\n",
    "\n",
    "# filter list for all folders containing data (folders that don't start with .)\n",
    "file_list_filtered = [s for s in os.listdir(d)\n",
    "                      if os.path.isdir(os.path.join(d, s)) & ~fnmatch.fnmatch(s, '.*')]\n",
    "\n",
    "# create pandas data frame for all the data\n",
    "\n",
    "df = None\n",
    "\n",
    "for category in file_list_filtered:\n",
    "    data_files = os.listdir('HMP_Dataset/' + category)\n",
    "\n",
    "    # create a temporary pandas data frame for each data file\n",
    "    for data_file in data_files:\n",
    "        if sample < 1.0:\n",
    "            if random.random() > sample:\n",
    "                print('Skipping: ' + data_file)\n",
    "                continue\n",
    "        print(data_file)\n",
    "        temp_df = spark.read. \\\n",
    "            option(\"header\", \"false\"). \\\n",
    "            option(\"delimiter\", \" \"). \\\n",
    "            csv('HMP_Dataset/' + category + '/' + data_file, schema=schema)\n",
    "\n",
    "        # create a column called \"source\" storing the current CSV file\n",
    "        temp_df = temp_df.withColumn(\"source\", lit(data_file))\n",
    "\n",
    "        # create a column called \"class\" storing the current data folder\n",
    "        temp_df = temp_df.withColumn(\"class\", lit(category))\n",
    "\n",
    "        if df is None:\n",
    "            df = temp_df\n",
    "        else:\n",
    "            df = df.union(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6afe503",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Lets write the dataf-rame to a file in \"CSV\" format, this will also take quite some time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e032ac46",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Path(data_dir + data_csv).exists():\n",
    "    shutil.rmtree(data_dir + data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3696103b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a3c31e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5310de21",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.write.option(\"header\", \"true\").csv(data_dir + data_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2aeb8e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Now we should have a CSV file with our contents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1.291986,
   "end_time": "2023-08-12T09:30:47.484220",
   "environment_variables": {},
   "exception": true,
   "input_path": "/home/davis/Private/main/input-hmp.ipynb",
   "output_path": "/home/davis/Private/main/input-hmp.ipynb",
   "parameters": {},
   "start_time": "2023-08-12T09:30:46.192234",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
